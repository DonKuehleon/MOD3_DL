<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Deep Learning in practice</title>
    <meta charset="utf-8" />
    <meta name="author" content="Stefan Kunz" />
    <script src="libs/header-attrs-2.5/header-attrs.js"></script>
    <link href="libs/xaringanExtra-extra-styles-0.2.4/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title-slide

.bg-text[

# Deep Learning in practice
### 
&lt;hr /&gt;

January, 14th

Ralf B. Sch√§fer &amp; Stefan Kunz

University of Koblenz-Landau

]

--

.center[
&lt;img src="Pictures/rbs_express.jpg" style="width: 20%"/&gt;
]

&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 20px;
    padding: 1em 4em 1em 4em;
}
.small {
  font-size: 15px;
}
&lt;/style&gt;






---

### Libraries &amp; frameworks for deep learning models

#### R packages that implement NN:

- *neuralnet*, *nnet*, *MXNet*, *tensorflow &amp; keras*, *torch*, *...*

#### Frameworks for training deep learning models:

- Developed by industry (Google, Facebook, ...) and research institutions

- Previously written code blocks have been re-used for new tools &amp; algorithms 

- Created with *Python*, *C++*, *Lua*, ...

- Mostly used with Python 

---

### Frameworks for training deep learning models:

- &lt;img src="Pictures/TensorFlowLogo.svg" style="width: 12%"/&gt;
  - Library for training and inference of deep neural networks developed for internal use by Google (2011)
  - R: https://tensorflow.rstudio.com/

--

- &lt;img src="Pictures/Pytorch.png" style="width: 16%"/&gt;
  - Developd by Facebook in 2016 (open-source) for *Python*  
  - R: torch package https://torch.mlverse.org/packages/

--

- &lt;img src="Pictures/Keras.png" style="width: 12%"/&gt;
  - Open-source high-level deep learning framework by Francois Chollet (written in *Python*)
  - Easy to use, supports tensorflow and other deep learning libraries as backend 
  - Keras now part of TensorFlow


???
_Keras: coding the model and the training, TensorFlow for high-performance data pipeline,_ 
_provides also backend to other frameworks (e.g. Theano)_
_Keras: in f10-15 lines of code a deep learning model can be defined and trained_

_Tensorflow source: https://upload.wikimedia.org/wikipedia/commons/1/11/TensorFlowLogo.svg_

_Mention that there are also frameworks for inference, e.g. to make predictions inside an app_

---

### Frameworks for training deep learning models

- &lt;img src="Pictures/TensorFlowLogo.svg" style="width: 12%"/&gt;

- &lt;img src="Pictures/Keras.png" style="width: 12%"/&gt;

- &lt;img src="Pictures/Pytorch.png" style="width: 16%"/&gt;


- Sharing models: Open Neural Network Exchange (ONNX)

???
ONNX: Standard format for machine learning models; provides conversion of models between frameworks
Computation graph models, definitions for operators and standard data types
Could use R with ONNX
There is also the https://github.com/rstudio/tensorflow and 
https://cran.r-project.org/web/packages/keras/vignettes/index.html
https://torch.mlverse.org/

---

### Small example of a single hidden layer Neural Network (NN)

- Will use the **nnet** package

- Acess through **caret** package


--

- Provides an uniform interface for a large variety of different modeling functions in R


```r
library(nnet)
library(caret)
caret::train(
  form,
  method = "nnet",
  tuneGrid,
  ...
)
```

--

- Overview of supported models: http://topepo.github.io/caret/train-models-by-tag.html


???
_Package by Max Kuhn_
_set of functions to streamlining predictive modelling_
_Tools for data splitting, pre-processing, feature selection, resampling, variable importance_
_uniform syntax_

---

### Small example of a single hidden layer NN

#### Nanopart data

* Data: nanoparticles measured with and without fulvic acid 


```r
# load Set_up script
source(file.path(getwd(), "R", "Set_up.R"))

# load data
nanopart &lt;- readRDS(file.path(data_in, "nanopart_preproc.RDS"))
dim(nanopart)
```

```
## [1] 171 120
```

---

### Small example of a single hidden layer NN

#### Nanopart data

- Variables: molecular masses with signal intensities 


```r
names(nanopart)[1:4]
```

```
## [1] "Mass_12_00" "Mass_15_01" "Mass_19_00" "Mass_24_00"
```

```r
summary(nanopart$Mass_12_00)
```

```
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   91121  157541  214411  208241  251010  437227
```

```r
summary(nanopart$fulvic_acid)
```

```
##  0  1 
## 90 81
```


---

### Small example of a single hidden layer NN

#### Data preparation nanopart

- Normalising the data either to `\([0, 1]\)` or scaling to `\(\mu = 0\)` and `\(\sigma = 1\)` to avoid the dominance of variables with large values

--


```r
# scaling (mean zero, sd 1)
stand_nanopart &lt;-
  scale(nanopart[, -ncol(nanopart)], 
        center = TRUE,
        scale = TRUE) %&gt;%
  as.data.frame() %&gt;%
  cbind(., "fulvic_acid" = nanopart[, "fulvic_acid"])

# check mean and sd
summary(stand_nanopart$Mass_12_00)
```

```
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -1.9774 -0.8560  0.1042  0.0000  0.7221  3.8660
```

```r
sd(stand_nanopart$Mass_12_00)
```

```
## [1] 1
```

???
_remember every input gets multiplied by the weights and then summed up at the neuron_
_the resulting value is then applied to the activation function_ 
_If data are not in the same range, large valued variables will dominate_
_If training long enough than weights would adjust for large valued variables, however this is not desirable_

_uniform and highly non linear variables - normalize_
_otherwise standardize_


---

### Small example of a single hidden layer NN
#### Data preparation nanopart

- Divide data into test and training data

- *caret::createDataParatition()* does sample from within factors


```r
# index &lt;- sample(1:nrow(stand_nanopart), 
# ceiling(0.75 * nrow(stand_nanopart)))
# train &lt;- stand_nanopart[index, ]
# test &lt;- stand_nanopart[-index, ]

ind &lt;- createDataPartition(stand_nanopart$fulvic_acid, p = 0.75)
train &lt;- stand_nanopart[ind[[1]], ]
test &lt;- stand_nanopart[-ind[[1]], ]
```


---

### Small example of a single hidden layer NN 
#### Training


```r
str(train$fulvic_acid)
```

```
##  Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 1 ...
```

```r
nn_nanopart &lt;- train(
  fulvic_acid ~ .,
  data = train,
  method = "nnet", 
  tuneGrid = expand.grid(size = 5, # number of hidden layers
                         decay = 5e-4), # regularization parameter
  maxit = 100, 
  metric = "Accuracy"
)
```

```
## # weights:  606
## initial  value 93.181713 
## iter  10 value 20.530877
## iter  20 value 3.467579
## iter  30 value 0.765950
## iter  40 value 0.515831
## iter  50 value 0.434554
## iter  60 value 0.384603
## iter  70 value 0.354299
## iter  80 value 0.325228
## iter  90 value 0.302063
## iter 100 value 0.279141
## final  value 0.279141 
## stopped after 100 iterations
## # weights:  606
## initial  value 87.510455 
## iter  10 value 36.047011
## iter  20 value 22.905789
## iter  30 value 4.658219
## iter  40 value 1.002085
## iter  50 value 0.871346
## iter  60 value 0.760866
## iter  70 value 0.663048
## iter  80 value 0.581185
## iter  90 value 0.530089
## iter 100 value 0.437502
## final  value 0.437502 
## stopped after 100 iterations
## # weights:  606
## initial  value 88.807845 
## iter  10 value 24.097203
## iter  20 value 2.434741
## iter  30 value 0.413691
## iter  40 value 0.346468
## iter  50 value 0.294477
## iter  60 value 0.265491
## iter  70 value 0.246270
## iter  80 value 0.236900
## iter  90 value 0.223787
## iter 100 value 0.212910
## final  value 0.212910 
## stopped after 100 iterations
## # weights:  606
## initial  value 92.311847 
## iter  10 value 33.516173
## iter  20 value 22.221448
## iter  30 value 3.220196
## iter  40 value 2.804298
## iter  50 value 2.687163
## iter  60 value 0.715327
## iter  70 value 0.478234
## iter  80 value 0.452913
## iter  90 value 0.427532
## iter 100 value 0.387675
## final  value 0.387675 
## stopped after 100 iterations
## # weights:  606
## initial  value 93.580694 
## iter  10 value 20.211214
## iter  20 value 8.185255
## iter  30 value 7.901725
## iter  40 value 7.399546
## iter  50 value 4.818774
## iter  60 value 0.925885
## iter  70 value 0.813598
## iter  80 value 0.711774
## iter  90 value 0.587813
## iter 100 value 0.460806
## final  value 0.460806 
## stopped after 100 iterations
## # weights:  606
## initial  value 93.146142 
## iter  10 value 20.098864
## iter  20 value 5.401826
## iter  30 value 0.850099
## iter  40 value 0.537729
## iter  50 value 0.464607
## iter  60 value 0.404200
## iter  70 value 0.380198
## iter  80 value 0.366011
## iter  90 value 0.336356
## iter 100 value 0.298760
## final  value 0.298760 
## stopped after 100 iterations
## # weights:  606
## initial  value 87.622276 
## iter  10 value 19.219275
## iter  20 value 1.750659
## iter  30 value 0.913198
## iter  40 value 0.783064
## iter  50 value 0.667251
## iter  60 value 0.623266
## iter  70 value 0.557019
## iter  80 value 0.504209
## iter  90 value 0.447537
## iter 100 value 0.410388
## final  value 0.410388 
## stopped after 100 iterations
## # weights:  606
## initial  value 90.554730 
## iter  10 value 59.558723
## iter  20 value 16.334099
## iter  30 value 1.933113
## iter  40 value 1.192415
## iter  50 value 0.826606
## iter  60 value 0.624886
## iter  70 value 0.460872
## iter  80 value 0.404792
## iter  90 value 0.360800
## iter 100 value 0.334087
## final  value 0.334087 
## stopped after 100 iterations
## # weights:  606
## initial  value 107.111102 
## iter  10 value 45.325090
## iter  20 value 10.376550
## iter  30 value 1.977840
## iter  40 value 0.964911
## iter  50 value 0.757765
## iter  60 value 0.605108
## iter  70 value 0.538268
## iter  80 value 0.494731
## iter  90 value 0.468092
## iter 100 value 0.445354
## final  value 0.445354 
## stopped after 100 iterations
## # weights:  606
## initial  value 123.910794 
## iter  10 value 41.952081
## iter  20 value 20.675701
## iter  30 value 8.844628
## iter  40 value 8.471217
## iter  50 value 7.466216
## iter  60 value 6.787709
## iter  70 value 5.427697
## iter  80 value 1.661677
## iter  90 value 0.612258
## iter 100 value 0.499912
## final  value 0.499912 
## stopped after 100 iterations
## # weights:  606
## initial  value 92.972356 
## iter  10 value 33.210821
## iter  20 value 3.376561
## iter  30 value 1.235979
## iter  40 value 1.010931
## iter  50 value 0.886725
## iter  60 value 0.823065
## iter  70 value 0.772157
## iter  80 value 0.737091
## iter  90 value 0.655278
## iter 100 value 0.561876
## final  value 0.561876 
## stopped after 100 iterations
## # weights:  606
## initial  value 95.458466 
## iter  10 value 34.679030
## iter  20 value 15.544754
## iter  30 value 7.525400
## iter  40 value 6.188878
## iter  50 value 0.567809
## iter  60 value 0.461336
## iter  70 value 0.399650
## iter  80 value 0.326468
## iter  90 value 0.287116
## iter 100 value 0.263139
## final  value 0.263139 
## stopped after 100 iterations
## # weights:  606
## initial  value 91.897833 
## iter  10 value 29.915126
## iter  20 value 24.750155
## iter  30 value 11.851465
## iter  40 value 11.403210
## iter  50 value 11.235723
## iter  60 value 11.095641
## iter  70 value 10.836856
## iter  80 value 10.690511
## iter  90 value 7.047163
## iter 100 value 0.864075
## final  value 0.864075 
## stopped after 100 iterations
## # weights:  606
## initial  value 99.686216 
## iter  10 value 37.669256
## iter  20 value 17.290303
## iter  30 value 4.047295
## iter  40 value 3.876879
## iter  50 value 1.086287
## iter  60 value 0.466304
## iter  70 value 0.415488
## iter  80 value 0.363774
## iter  90 value 0.327947
## iter 100 value 0.298860
## final  value 0.298860 
## stopped after 100 iterations
## # weights:  606
## initial  value 92.752435 
## iter  10 value 35.869258
## iter  20 value 18.809745
## iter  30 value 7.456963
## iter  40 value 1.848446
## iter  50 value 1.144496
## iter  60 value 0.871393
## iter  70 value 0.710542
## iter  80 value 0.546472
## iter  90 value 0.487189
## iter 100 value 0.431242
## final  value 0.431242 
## stopped after 100 iterations
## # weights:  606
## initial  value 113.669646 
## iter  10 value 27.519131
## iter  20 value 11.215761
## iter  30 value 9.856952
## iter  40 value 9.735408
## iter  50 value 4.848432
## iter  60 value 0.603340
## iter  70 value 0.520960
## iter  80 value 0.481928
## iter  90 value 0.453631
## iter 100 value 0.434928
## final  value 0.434928 
## stopped after 100 iterations
## # weights:  606
## initial  value 111.932160 
## iter  10 value 24.723658
## iter  20 value 11.070369
## iter  30 value 10.724398
## iter  40 value 10.568583
## iter  50 value 10.406681
## iter  60 value 1.141591
## iter  70 value 0.572869
## iter  80 value 0.512067
## iter  90 value 0.423419
## iter 100 value 0.391358
## final  value 0.391358 
## stopped after 100 iterations
## # weights:  606
## initial  value 103.070149 
## iter  10 value 34.546709
## iter  20 value 13.426669
## iter  30 value 8.140017
## iter  40 value 0.603764
## iter  50 value 0.517020
## iter  60 value 0.409621
## iter  70 value 0.343743
## iter  80 value 0.306947
## iter  90 value 0.271897
## iter 100 value 0.244545
## final  value 0.244545 
## stopped after 100 iterations
## # weights:  606
## initial  value 91.491697 
## iter  10 value 25.034715
## iter  20 value 2.973598
## iter  30 value 0.824819
## iter  40 value 0.710500
## iter  50 value 0.622785
## iter  60 value 0.554633
## iter  70 value 0.480604
## iter  80 value 0.425026
## iter  90 value 0.377026
## iter 100 value 0.352756
## final  value 0.352756 
## stopped after 100 iterations
## # weights:  606
## initial  value 84.673046 
## iter  10 value 20.794095
## iter  20 value 1.589354
## iter  30 value 1.100132
## iter  40 value 0.878836
## iter  50 value 0.767378
## iter  60 value 0.632502
## iter  70 value 0.564345
## iter  80 value 0.474101
## iter  90 value 0.402668
## iter 100 value 0.343509
## final  value 0.343509 
## stopped after 100 iterations
## # weights:  606
## initial  value 97.893657 
## iter  10 value 22.297910
## iter  20 value 1.158581
## iter  30 value 0.890736
## iter  40 value 0.721511
## iter  50 value 0.630595
## iter  60 value 0.567462
## iter  70 value 0.521812
## iter  80 value 0.502870
## iter  90 value 0.477453
## iter 100 value 0.416337
## final  value 0.416337 
## stopped after 100 iterations
## # weights:  606
## initial  value 94.703505 
## iter  10 value 32.218367
## iter  20 value 17.525845
## iter  30 value 6.795018
## iter  40 value 1.566955
## iter  50 value 0.823052
## iter  60 value 0.747383
## iter  70 value 0.635164
## iter  80 value 0.582949
## iter  90 value 0.546761
## iter 100 value 0.493806
## final  value 0.493806 
## stopped after 100 iterations
## # weights:  606
## initial  value 92.828470 
## iter  10 value 19.543183
## iter  20 value 9.634082
## iter  30 value 5.634074
## iter  40 value 0.664681
## iter  50 value 0.558658
## iter  60 value 0.504324
## iter  70 value 0.447928
## iter  80 value 0.411931
## iter  90 value 0.332400
## iter 100 value 0.291088
## final  value 0.291088 
## stopped after 100 iterations
## # weights:  606
## initial  value 82.962962 
## iter  10 value 40.122412
## iter  20 value 12.194521
## iter  30 value 8.158019
## iter  40 value 7.592840
## iter  50 value 7.526355
## iter  60 value 7.430360
## iter  70 value 7.367355
## iter  80 value 2.010155
## iter  90 value 0.594907
## iter 100 value 0.571801
## final  value 0.571801 
## stopped after 100 iterations
## # weights:  606
## initial  value 110.103620 
## iter  10 value 24.289154
## iter  20 value 4.672369
## iter  30 value 0.390115
## iter  40 value 0.347794
## iter  50 value 0.281545
## iter  60 value 0.250086
## iter  70 value 0.239601
## iter  80 value 0.227999
## iter  90 value 0.213814
## iter 100 value 0.202377
## final  value 0.202377 
## stopped after 100 iterations
## # weights:  606
## initial  value 96.723834 
## iter  10 value 13.332756
## iter  20 value 1.053899
## iter  30 value 0.900014
## iter  40 value 0.804353
## iter  50 value 0.720397
## iter  60 value 0.646199
## iter  70 value 0.594090
## iter  80 value 0.532426
## iter  90 value 0.460385
## iter 100 value 0.394269
## final  value 0.394269 
## stopped after 100 iterations
```

???
_mention tuning and preprocessing_
_linout argument: output layer linear or sigmoid_

---

### Small example of a single hidden layer NN 


```r
NeuralNetTools::plotnet(nn_nanopart)
```

![](Presentation_DL_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;


???
_Nr of weights = parameters?_

---
### Small example of a single hidden layer NN 
#### Predictions


```r
# training data
result_train &lt;- predict(nn_nanopart, newdata = train)
conf_train &lt;- caret::confusionMatrix(result_train, 
                                     train$fulvic_acid)

conf_train
```

```
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  0  1
##          0 68  0
##          1  0 61
##                                      
##                Accuracy : 1          
##                  95% CI : (0.9718, 1)
##     No Information Rate : 0.5271     
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  
##                                      
##                   Kappa : 1          
##                                      
##  Mcnemar's Test P-Value : NA         
##                                      
##             Sensitivity : 1.0000     
##             Specificity : 1.0000     
##          Pos Pred Value : 1.0000     
##          Neg Pred Value : 1.0000     
##              Prevalence : 0.5271     
##          Detection Rate : 0.5271     
##    Detection Prevalence : 0.5271     
##       Balanced Accuracy : 1.0000     
##                                      
##        'Positive' Class : 0          
## 
```

---

### Small example of a single hidden layer NN 
#### Predictions


```r
# test data:
results_test &lt;- predict(nn_nanopart, newdata=test)
conf_test &lt;- confusionMatrix(results_test, 
                             test$fulvic_acid)

conf_test
```

```
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  0  1
##          0 21  0
##          1  1 20
##                                           
##                Accuracy : 0.9762          
##                  95% CI : (0.8743, 0.9994)
##     No Information Rate : 0.5238          
##     P-Value [Acc &gt; NIR] : 6.286e-11       
##                                           
##                   Kappa : 0.9524          
##                                           
##  Mcnemar's Test P-Value : 1               
##                                           
##             Sensitivity : 0.9545          
##             Specificity : 1.0000          
##          Pos Pred Value : 1.0000          
##          Neg Pred Value : 0.9524          
##              Prevalence : 0.5238          
##          Detection Rate : 0.5000          
##    Detection Prevalence : 0.5000          
##       Balanced Accuracy : 0.9773          
##                                           
##        'Positive' Class : 0               
## 
```

___

### Small example of a single hidden layer NN 
#### Predictions


```r
# get the probabilities for each class in the test set
predict(nn_nanopart,
        newdata = test,
        type = 'prob')
```


---
### Small example of a single hidden layer NN 
#### Predictions

- Prediction accuracy on test and traing dataset is 1

- Model has potentially overfitted the data

- Number of weights?



```r
# input layer nodes * hidden layer nodes + 
# hidden layer nodes * output layer nodes +
# bias (1) * hidden layers + bias (1) * output layer
119*5 + 5*1 + 1*5 + 1*1
```

```
## [1] 606
```

```r
# confirm with
nn_nanopart$finalModel
```

```
## a 119-5-1 network with 606 weights
## inputs: Mass_12_00 Mass_15_01 Mass_19_00 Mass_24_00 Mass_26_01 Mass_27_01 Mass_27_03 Mass_28_99 Mass_31_97 Mass_31_99 Mass_33_00 Mass_36_00 Mass_38_02 Mass_39_01 Mass_39_03 Mass_40_02 Mass_41_02 Mass_42_00 Mass_42_99 Mass_43_97 Mass_45_98 Mass_45_99 Mass_48_00 Mass_49_01 Mass_50_01 Mass_50_99 Mass_51_03 Mass_51_99 Mass_52_03 Mass_53_04 Mass_60_98 Mass_61_99 Mass_62_97 Mass_64_00 Mass_65_01 Mass_65_04 Mass_66_00 Mass_66_05 Mass_66_98 Mass_67_97 Mass_68_02 Mass_69_00 Mass_69_04 Mass_71_02 Mass_74_01 Mass_75_96 Mass_76_03 Mass_77_97 Mass_78_01 Mass_78_05 Mass_78_96 Mass_82_02 Mass_83_01 Mass_84_00 Mass_84_05 Mass_85_00 Mass_87_95 Mass_87_99 Mass_88_04 Mass_89_00 Mass_89_95 Mass_90_00 Mass_90_05 Mass_91_02 Mass_91_05 Mass_93_00 Mass_95_93 Mass_96_00 Mass_96_05 Mass_96_93 Mass_96_97 Mass_97_02 Mass_98_02 Mass_99_00 Mass_99_04 Mass_101_05 Mass_102_00 Mass_102_05 Mass_105_94 Mass_109_01 Mass_110_02 Mass_111_02 Mass_112_03 Mass_113_01 Mass_113_06 Mass_114_02 Mass_115_02 Mass_116_04 Mass_117_04 Mass_118_04 Mass_119_99 Mass_121_03 Mass_121_99 Mass_123_03 Mass_126_91 Mass_127_09 Mass_134_02 Mass_134_07 Mass_135_93 Mass_136_03 Mass_136_95 Mass_141_10 Mass_143_10 Mass_155_11 Mass_156_91 Mass_157_90 Mass_158_91 Mass_161_04 Mass_169_02 Mass_169_13 Mass_171_13 Mass_175_86 Mass_176_87 Mass_177_09 Mass_177_89 Mass_183_03 Mass_183_15 Mass_185_05 Mass_199_16 
## output(s): .outcome 
## options were - entropy fitting  decay=5e-04
```

---

### Small example of a single hidden layer NN 

Overfitting: 

- Typically: 
  - Very good performance on training data, bad performance on test data
  (i.e. model has learned the noise in the training data)

  - Many parameters and small datasets 

--

- Circumvent: Regularization &amp; DropOut

--

- In practice: NN in deep learning have often more parameters than training samples

???
_DropOut similarly to LASSO_
_Maybe even the network structure prevents overfitting - ongoing research (Zhang et al. 2017)_
_Many parameters also means high flexibility_

---

### Small example of a single hidden layer NN 
#### Relative variable importance

- Deconstructing model weights: identification of all weighted connections between an input node and the output node(s)


```r
VI &lt;- garson(nn_nanopart)
VI$data[VI$data$x_names == "Mass_39_03", ]
```

```
##       rel_imp    x_names
## 15 0.01119426 Mass_39_03
```


---

### Small example of a single hidden layer NN 
#### Task: 

Previous analysis of the data suggested that the following masses seem to
be important to distinguish samples that contained fulvic acid from samples
without fulvic acid:
**Mass_39_03**, 
**Mass_38_02**, 
**Mass_65_04**, 
**Mass_53_04**, 
**Mass_45_98**

1. Create a neural network classifier with only these 5 parameters, predicting whether
a sample contained fulvic acid or not. *Bonus: vary the number of hidden units between 2 and 5.*

2. How many weights has the neural network? 

3. Compare the results with a logistic regression model



---
### Deep learning (DL) algorithms are data hungry

--

- DL algorithms can only detect what they have previously seen

- The bigger the dataset, the better the classification accuracy (Marcus, 2018)

- Each classification category should be sufficiently represented in the training data  


---
### Deep learning (DL) algorithms are data hungry

- One possible solution:  **Transfer learning**

--

- Extract knowledge from *source task* (e.g. a pretrained model) and apply to *target task*

--
&lt;img src="Pictures/TL_explan.png" style="width: 100%"/&gt;


???
_That means using an existing model, trained on a large dataset and tuning it to our dataset_



---
### Transfer learning

#### Advantages

- Reduced training time

- Smaller datasets (hundreds to thousands of samples)
--


#### Prerequisites: 

- Availability of pretrained models  



&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Task &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Examples &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Image Classification &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; ResNet-152 (2015), MobileNet (2017) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Text classification &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; BERT (2018), XLNet (2019) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Image segmentation &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; U-Net (2015), DeepLabV3 (2018) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Image translation &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Pix2Pix (2017) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Object detection &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; YOLO9000 (2016), Mask R-CNN (2017) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Speech generation &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; WaveNet (2016) &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

???
_Say here: 

Model architecture: Layers, graph of nodes and edges
1) input 2) getting output 3) comparison with labels/predictions vs expectations
4) propagating magnitude of error back to the model so that it can learn

Result of training: Weights of the nodes

Types of nodes: different themes of model architectures -&gt; CNNs, RNNs, GANs

We will use MobileNet(), briefly introduce: Developed and trained by google for mobile devices (limited computational power and space). Could google and find model as well as accuracy metrics 

_TODO: What is image segmentation and image translation?_




---
### Transfer Learning

#### Prerequisites: 

- Availability of pretrained models  

--

- Large annotated datasets (e.g. ImageNet)

&lt;img src="Pictures/imagenet.png" style="width: 75%"/&gt;


.footnote[.small[*http://image-net.org/explore*]]


---



### Transfer Learning

#### Prerequisites: 

- Availability of pretrained models 

- Large annotated datasets (e.g. ImageNet)

- Developments in computational power, i.e., faster and cheaper GPUs

- Availability of algorithms (model architecture, optimizers,...)

---



### Transfer Learning using CNNs for image classification


- Remove last few layers and "freeze" generic layers 

&lt;img src="Pictures/cnn_transfer_learning.png" style="width: 75%"/&gt;



.footnote[.small[*Koul, Ganju &amp; Kasam (2020)*]]


???
_First layers more general, middle-layers already specific, last layers very specific_
_freeze: weights stay the same_
_say that features relate to characteristics in the pictures not to the variables_

---

### Finetuning 

- Unfreeze few of the frozen layers

- Dependent on the amount of task-specific data

&lt;img src="Pictures/finetuning.png" style="width: 75%"/&gt;


.footnote[.small[*Koul, Ganju &amp; Kasam (2020)*]]

???
_allowing more weights to change_

---



### Examples in ecology 


- Transfer learning &amp; CNNs to identify species of *Chironomidae* (Milo≈°eviƒá et al. 2019)

&lt;img src="Pictures/chironomidae_classifier.png" style="width: 65%"/&gt;


???
_What is shown here?_

---

### Examples in ecology 

#### Using transfer learning:

- CNNs to identify species of *Chironomidae* (Milo≈°eviƒá et al. 2019)

--

- Object detection (R-CNN/YOLO) to label camera trap images (Schneider et al. 2018)

--

#### Overview on recent studies:


- Review: Christin et al. (2019) - Applications of deep learning in ecology


???
_Most Papers very recently published in this field (&gt; 2017)_
_Usage not only for image classification -&gt; think outside of the box (Max Joseph)_

---

### Image classification example in *Python* using transfer learning


- We are using an existing pretrained CNN (*MobileNet()*)

- Its weights are trained on a large annotated image database (ImageNet)

- We will "freeze" most of the generic layers

- We will exchange few of the last layers (specific) to customize the model
for our classification task

--

- We will classify:

&lt;img src="Pictures/cat_vs_dog.jpeg" style="width: 35%"/&gt;

---

### Applications

.center[

Art: https://deepart.io/

&lt;img src="Pictures/rbs_express.jpg" style="width: 20%"/&gt;

]

--

.center[

&lt;img src="Pictures/rbs.png" style="width: 20%"/&gt;
]



---
### References

#### Papers:

- Christin, S., Hervet, √â., &amp; Lecomte, N. (2019). Applications for deep learning in ecology. Methods in Ecology and Evolution, 10(10), 1632‚Äì1644. https://doi.org/10.1111/2041-210X.13256


- Marcus, G. (2018). Deep Learning: A Critical Appraisal.
ArXiv:1801.00631. http://arxiv.org/abs/1801.00631

- Milo≈°eviƒá, D., Milosavljeviƒá, A., Prediƒá, B., Medeiros, A. S., Saviƒá-Zdravkoviƒá, D., Stojkoviƒá Piperac, M., Kostiƒá, T., Spasiƒá, F., &amp; Leese, F. (2020). Application of deep learning in aquatic bioassessment: Towards automated identification of non-biting midges. Science of The Total Environment, 711, 135160. https://doi.org/10.1016/j.scitotenv.2019.135160

- Schneider, S., Taylor, G. W., &amp; Kremer, S. C. (2018). Deep Learning Object Detection Methods for Ecological Camera Trap Data. arXiv:1803.10842 [cs]. http://arxiv.org/abs/1803.10842


---

### References

#### Books

Koul A., Ganja S., Kasam M. (2020). Practical Deep Learning for Cloud, Mobile, and Edge. O'Reiley. ISBN: 978-1-392-03486-5


---

### References

#### Images

- Guitar:
https://c1.zzounds.com/media/productmedia/fit,2018by3200/quality,85/1_Full_Straight_Front_NA-b2c2db10f401d973c12a0e86294f5f7c.jpg
 

- Ukulele:
https://www.picklepwns.com/wp-content/uploads/2019/02/Best-ukulele-chords.jpg

- ImageNet screenshot: 
http://image-net.org/explore

- RBS: 
https://i1.rgstatic.net/ii/profile.image/272184648138758-1441905285191_Q512/Ralf_Bernhard_Schaefer.jpg

- Cats vs dogs: 
https://gsurma.medium.com/image-classifier-cats-vs-dogs-with-convolutional-neural-networks-cnns-and-google-colabs-4e9af21ae7a8

---





    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "solarized-light",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
