---
title: "Deep Learning"
subtitle: "..."
author: "Stefan Kunz"
# date: "2016/12/12"
institute: "AG Landscape Ecology"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: solarized-light
      highlightLines: true
      countIncrementalSlides: false
---
<style type="text/css">
.remark-slide-content {
    font-size: 18px;
    padding: 1em 4em 1em 4em;
}
.small {
  font-size: 15px;
}
</style>


```{r xaringan-themer, include=FALSE, warning=FALSE}
library(knitr)
library(xaringanthemer)
# style_duo(primary_color = "#43418A", 
#           secondary_color = "#F97B64")
style_solarized_light(header_color = "steelblue")
```

```{r xaringan-extra-styles, include = FALSE}
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
```

General: 
- Weights
- Activation function
- Loss function
- Feedforward and backpropagation
- Model training

Few introductory words to: 
- Building NN yourself
- Using existing architectures and models 
- Name some frameworks
- Name some existing Models that can be used 
- Training and Test data!

---

# Deep Learning in practice

???
_some introductionary picture? 

---

## Applications

- Examples first!


---

## Deep Learning General

- Machine Learning: 

  - Generate predictive models by learning patterns in data
  - Assume a pre-specified representation

--

- Deep Learning: 
  - Powerful framework for supervised learning
  - *Automatically* detect and extract features in data
  - Goal: prediction
  - Requires only annotated training data

  $\rightarrow$  Core: function approximation

---

### Deep learning models are data hungry
One possible solution: 
--
**Transfer learning**
--

- Extract knowledge from *source task* (e.g. a pretrained model) and apply to *target task*
<img src="Pictures/TL_explan.png" style="width: 100%"/>

???
_That means using an existing model, trained on a large dataset and tuning it to our dataset_



---
### Transfer learning

#### Advantages

- Reduced training time

- Smaller datasets (hundreds to thousands of samples)
--


#### Prerequisites: 

- Availability of pretrained models  

```{r, include=FALSE}
mod_architectures <- data.frame(
  Task = c(
    "Image Classification",
    "Text classification",
    "Image segmentation",
    "Image translation",
    "Object detection",
    "Speech generation"
  ),
  `Examples` = c(
    "ResNet-152 (2015), MobileNet (2017)",
    "BERT (2018), XLNet (2019)",
    "U-Net (2015) DeepLabV3 (2018)",
    "Pix2Pix (2017)",
    "YOLO9000 (2016), Mask R-CNN (2017)",
    "WaveNet (2016)"
  )
)
```

```{r, echo = FALSE}
knitr::kable(x = mod_architectures, format = "html")
```

???
_Say here: 

Model architecture: Layers, graph of nodes and edges
1) input 2) getting output 3) comparison with labels/predictions vs expectations
4) propagating magnitude of error back to the model so that it can learn

Result of training: Weights of the nodes

Types of nodes: different themes of model architectures -> CNNs, RNNs, GANs

We will use MobileNet(), briefly introduce: Developed and trained by google for mobile devices (limited computational power and space). Could google and find model as well as accuracy metrics 

_TODO: What is image segmentation and image translation?_




---
### Transfer Learning
<hr width = 750, align = "left">

#### Prerequisites: 

- Availability of pretrained models  

--

- Large annotated datasets (e.g. ImageNet)

<img src="Pictures/imagenet.png" style="width: 75%"/>


.footnote[.small[*http://image-net.org/explore*]]


---



### Transfer Learning

#### Prerequisites: 

- Availability of pretrained models 

- Large annotated datasets (e.g. ImageNet)

- Developments in computational power, i.e., faster and cheaper GPUs

- Availability of algorithms (model architecture, optimizers,...)

---



### Transfer Learning using CNNs


- Remove last few layers and "freeze" generic layers 

<img src="Pictures/cnn_transfer_learning.png" style="width: 75%"/>



.footnote[.small[*Koul, Ganju & Kasam (2020)*]]


???
_First layers more general, middle-layers already specific, last layers very specific_
_freeze: weights stay the same_

---

### Finetuning 

- Unfreezing few of the frozen layers

- Dependent on the amount of task-specific data

<img src="Pictures/finetuning.png" style="width: 75%"/>


.footnote[.small[*Koul, Ganju & Kasam (2020)*]]

???
_allowing more weights to change_

---



### Examples in ecology 


- Transfer learning & CNNs to identify species of *Chironomidae* 

<img src="Pictures/chironomidae_classifier.png" style="width: 50%"/>

--

- Transfer learning & Object detection (R-CNN/YOLO) to label camera trap images $^2$


.footnote[.small[*1 - Milošević et. al (2019); 
2 - Schneider et. al (2018)*
]] 



---

### Frameworks for training deep learning models

- TensorFlow: 
  - Developed initially for internal use by Google (2011)
  - Written in *Python* and *C++*
  - Difficult to use initially

- Keras: 
  - Open-source framework by Francois Chollet (written in *Python*)
  - Easy to use, supports other deep learning libraries as backend 
  - Keras now part of TensorFlow

- PyTorch: 
  - Developd by Facebook in 2016 (open-source) for *Python*
  - Easy to use

- Sharing models: Open Neural Network Exchange (ONNX)


???
Keras: coding the model and the training, TensorFlow for high-performance data pipeline 
provieds also backend to other frameworks (e.g. Theano)

ONNX: Standard format for machine learning models; provides conversion of models between frameworks
Computation graph models, definitions for operators and standard data types
Could use R with ONNX
There is also the https://github.com/rstudio/tensorflow and 
https://cran.r-project.org/web/packages/keras/vignettes/index.html
https://torch.mlverse.org/


Mention that there are also frameworks for inference, e.g. to make predictions inside an app 
---


Short recap at the end of the presentation, 
so we are using an existing pretrained model (MobileNet(), CNN), 
i.e. its weights trained on a large annotated image database
Will change few of the last layers (specific) to customize the model
for our classification task

---

### Basics of Deep Feedforward networks

- Goal: approximate some function $f$ 

--
- Mapping of some input x to an output y (e.g. a category)

$y = f(x, \theta)$ ; $\theta = parameters$

--
- Learns the value of the parameters $\theta$ that result in the best function approximation

--

- **Feedforward**: Information flows from $x$, through $f$ to output $y$

--

- **Networks**: Combine many different functions: 


$f^{(1)}, f^{(2)}, f^{(3)} \rightarrow f(x) f^{(3)}(f^{(2)}(f^{(1)}(x)))$

$f^{(1)} \rightarrow$ first layer, $f^{(2)} \rightarrow$ second layer, ... 

--

- Length of the chain: **Depth** of the model

--

- **Neural**: Inspired by the function of the brain, specifically the neuron

???
_No feedback connections in which outputs of the model are fed back into itself
_recurrent neural networks have feedback connections

_Deep Learnings vs shallow learning

_Only loosly inspired by neuroscience and the working of a neuron


---

### Basics of Deep forward networks 

Add here picture with Deep forward network structure (input, hidden and output layers)
then come to discussion about relationship to neuroscience

- Hidden layer: vectors
- Each element in the vector can be seen as a "neuron" (units)
- Each unit receives input from other units and computes its own activation value

- Conclusion: Rather think of feedforward networks as *function approximation machines* !

---

### Relationship to statistics

*"Neural networks are merely regression models with transformed predictors" (J. Hoeting)*

--

- Linear regression: 
  - Easy to fit, known assumptions
  - Limited to linear functions (cannot understand interaction between two input variables)

--
- Extension to represent nonlinear functions: apply linear model to transformed input 
$\phi(x)$; $\phi$ nonlinear transformation

- Nonparametric regression: Polynomial regression, regression splines
???
_splines apply transformation to x_
_Doesn't work well for many predictors_
_User has to decide about the transformation_

--

- Deep learning: learns $\phi$ based on the data!


---
### Examples 
#### General 
#### Ecology

---
### How to use in Ecology?

---
### Example of transfer learning




