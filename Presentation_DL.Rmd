---
title: "Deep Learning in practice"
subtitle: ""
author: "Stefan Kunz"
# date: "2016/12/12"
institute: "AG Landscape Ecology"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: solarized-light
      highlightLines: true
      countIncrementalSlides: false
      # countdown: 60000
    seal: false
---

class: title-slide

.bg-text[

# Deep Learning in practice
### 
<hr />

January, 14th

Ralf B. Sch√§fer & Stefan Kunz

University of Koblenz-Landau

]

--

.center[
<img src="Pictures/rbs_express.jpg" style="width: 20%"/>
]

<style type="text/css">
.remark-slide-content {
    font-size: 20px;
    padding: 1em 4em 1em 4em;
}
.small {
  font-size: 15px;
}
</style>


```{r xaringan-themer, include=FALSE, warning=FALSE}
library(knitr)
library(xaringanthemer)
library(xaringanExtra)
# library(showtext)
# style_duo(primary_color = "#43418A", 
#           secondary_color = "#F97B64")
style_solarized_light(header_color = "steelblue")
```

```{r xaringan-extra-styles, include = FALSE}
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
```

---

### Small example of a single hidden layer Neural Network (NN)

--
- R packages that implement NN: *neuralnet*, **nnet**, *MXNet*, *tensorflow & keras*, *torch*, *...*

--

- Acess through **caret** package


--

- Provides an uniform interface for a large variety of different modeling functions in R

```{r, eval = FALSE}
library(nnet)
library(caret)
caret::train(
  form,
  method = "nnet",
  tuneGrid,
  ...
)
```

--

- Overview of supported models: http://topepo.github.io/caret/train-models-by-tag.html


???
_Package by Max Kuhn_
_set of functions to streamlining predictive modelling_
_Tools for data splitting, pre-processing, feature selection, resampling, variable importance_
_uniform syntax_

---

### Small example of a single hidden layer NN

#### Nanopart data

* Data: nanoparticles measured with and without fulvic acid 

```{r}
# load Set_up script
source(file.path(getwd(), "R", "Set_up.R"))

# load data
nanopart <- readRDS(file.path(data_in, "nanopart_preproc.RDS"))
dim(nanopart)
```

---

### Small example of a single hidden layer NN

#### Nanopart data

- Variables: molecular masses with signal intensities 

```{r}
names(nanopart)[1:4]
summary(nanopart$Mass_12_00)
summary(nanopart$fulvic_acid)
```


---

### Small example of a single hidden layer NN

#### Data preparation nanopart

- Scaling the data either to $[0, 1]$ or $\mu = 0$ and $\sigma = 1$ to avoid the dominance of variables with large values

```{r}
# scaling (mean zero, sd 1)
stand_nanopart <-
  scale(nanopart[, -ncol(nanopart)], 
        center = TRUE,
        scale = TRUE) %>%
  as.data.frame() %>%
  cbind(., "fulvic_acid" = nanopart[, "fulvic_acid"])

# check mean and sd
summary(stand_nanopart$Mass_12_00)
sd(stand_nanopart$Mass_12_00)
```

???
_remember every input gets multiplied by the weights and then summed up at the neuron_
_the resulting value is then applied to the activation function_ 
_If data are not in the same range, large valued variables will dominate_
_If training long enough than weights would adjust for large valued variables, however this is not desirable_

_uniform and highly non linear variables - normalize_
_otherwise standardize_


---

### Small example of a single hidden layer NN
#### Data preparation nanopart

- Divide data into test and training data

- *caret::createDataParatition()* does sample from within factors

```{r}
# index <- sample(1:nrow(stand_nanopart), 
# ceiling(0.75 * nrow(stand_nanopart)))
# train <- stand_nanopart[index, ]
# test <- stand_nanopart[-index, ]

ind <- createDataPartition(stand_nanopart$fulvic_acid, p = 0.75)
train <- stand_nanopart[ind[[1]], ]
test <- stand_nanopart[-ind[[1]], ]
```


---

### Small example of a single hidden layer NN 
#### Training

```{r} 
# eval = FALSE
nn_nanopart <- train(
  fulvic_acid ~ .,
  data = train,
  method = "nnet", 
  tuneGrid = expand.grid(size = 5, # number of hidden layers
                         decay = 5e-4), # regularization parameter
  maxit = 100, 
  metric = "Accuracy"
)
```

???
_mention tuning and preprocessing_

---

### Small example of a single hidden layer NN 

```{r}
NeuralNetTools::plotnet(nn_nanopart)
```


???
_Nr of weights = parameters?_

---
### Small example of a single hidden layer NN 
#### Predictions

```{r}
# training data
result_train <- predict(nn_nanopart, newdata = train)
conf_train <- caret::confusionMatrix(result_train, 
                                     train$fulvic_acid)

conf_train
```

---

### Small example of a single hidden layer NN 
#### Predictions

```{r}
# test data:
results_test <- predict(nn_nanopart, newdata=test)
conf_test <- confusionMatrix(results_test, 
                             test$fulvic_acid)

conf_test
```

___

### Small example of a single hidden layer NN 
#### Predictions

```{r, eval = FALSE}
# get the probabilities for each class in the test set
predict(nn_nanopart,
        newdata = test,
        type = 'prob')
```


---
### Small example of a single hidden layer NN 
#### Predictions

- Prediction accuracy on test and traing dataset is 1

- Model has potentially overfitted the data

- Number of weights?


```{r}
# input layer nodes * hidden layer nodes + 
# hidden layer nodes * output layer nodes +
# bias (1) * hidden layers + bias (1) * output layer
119*5 + 5*1 + 1*5 + 1*1

# confirm with
nn_nanopart$finalModel
```

---

### Small example of a single hidden layer NN 

Overfitting: 

- Typically: 
  - Very good performance on training data, bad performance on test data
  (i.e. model has learned the noise in the training data)

  - Many parameters and small datasets 

--

- Circumvent: Regularization & DropOut

--

- In practice: NN in deep learning have often more parameters than training samples

???
_DropOut similarly to LASSO_
_Maybe even the network structure prevents overfitting - ongoing research (Zhang et al. 2017)_
_Many parameters also means high flexibility_

---

### Small example of a single hidden layer NN 
#### Relative variable importance

- Deconstructing model weights: identification of all weighted connections between an input node and the output node(s)

```{r}
VI <- garson(nn_nanopart)
VI$data[VI$data$x_names == "Mass_39_03", ]
```


---

### Small example of a single hidden layer NN 
#### Task: 

Previous analysis of the data suggested that the following masses seem to
be important to distinguish samples that contained fulvic acid from samples
without fulvic acid:
**Mass_39_03**, 
**Mass_38_02**, 
**Mass_65_04**, 
**Mass_53_04**, 
**Mass_45_98**

1. Create a neural network classifier with only these 5 parameters, predicting whether
a sample contained fulvic acid or not. *Bonus: vary the number of hidden units between 2 and 5.*

2. How many weights has the neural network? 

3. Compare the results with a logistic regression model



---
### Deep learning models are data hungry

One possible solution: 
--
**Transfer learning**
--

- Extract knowledge from *source task* (e.g. a pretrained model) and apply to *target task*

--
<img src="Pictures/TL_explan.png" style="width: 100%"/>

???
_That means using an existing model, trained on a large dataset and tuning it to our dataset_



---
### Transfer learning

#### Advantages

- Reduced training time

- Smaller datasets (hundreds to thousands of samples)
--


#### Prerequisites: 

- Availability of pretrained models  

```{r, include=FALSE}
mod_architectures <- data.frame(
  Task = c(
    "Image Classification",
    "Text classification",
    "Image segmentation",
    "Image translation",
    "Object detection",
    "Speech generation"
  ),
  `Examples` = c(
    "ResNet-152 (2015), MobileNet (2017)",
    "BERT (2018), XLNet (2019)",
    "U-Net (2015), DeepLabV3 (2018)",
    "Pix2Pix (2017)",
    "YOLO9000 (2016), Mask R-CNN (2017)",
    "WaveNet (2016)"
  )
)
```

```{r, echo = FALSE}
knitr::kable(x = mod_architectures, format = "html")
```

???
_Say here: 

Model architecture: Layers, graph of nodes and edges
1) input 2) getting output 3) comparison with labels/predictions vs expectations
4) propagating magnitude of error back to the model so that it can learn

Result of training: Weights of the nodes

Types of nodes: different themes of model architectures -> CNNs, RNNs, GANs

We will use MobileNet(), briefly introduce: Developed and trained by google for mobile devices (limited computational power and space). Could google and find model as well as accuracy metrics 

_TODO: What is image segmentation and image translation?_




---
### Transfer Learning

#### Prerequisites: 

- Availability of pretrained models  

--

- Large annotated datasets (e.g. ImageNet)

<img src="Pictures/imagenet.png" style="width: 75%"/>


.footnote[.small[*http://image-net.org/explore*]]


---



### Transfer Learning

#### Prerequisites: 

- Availability of pretrained models 

- Large annotated datasets (e.g. ImageNet)

- Developments in computational power, i.e., faster and cheaper GPUs

- Availability of algorithms (model architecture, optimizers,...)

---



### Transfer Learning using CNNs


- Remove last few layers and "freeze" generic layers 

<img src="Pictures/cnn_transfer_learning.png" style="width: 75%"/>



.footnote[.small[*Koul, Ganju & Kasam (2020)*]]


???
_First layers more general, middle-layers already specific, last layers very specific_
_freeze: weights stay the same_
_say that features relate to characteristics in the pictures not to the variables_

---

### Finetuning 

- Unfreeze few of the frozen layers

- Dependent on the amount of task-specific data

<img src="Pictures/finetuning.png" style="width: 75%"/>


.footnote[.small[*Koul, Ganju & Kasam (2020)*]]

???
_allowing more weights to change_

---



### Examples in ecology 


- Transfer learning & CNNs to identify species of *Chironomidae* 

<img src="Pictures/chironomidae_classifier.png" style="width: 60%"/>



.footnote[.small[Milo≈°eviƒá et. al (2019)
]] 

???
_What is shown here?_

---

### Examples in ecology 

#### Using transfer learning:

- CNNs to identify species of *Chironomidae* $^1$

--

- Object detection (R-CNN/YOLO) to label camera trap images $^2$

--

#### Other recent studies:

- Tracking of global fisheries $^3$

--

- Parameterisation of abundance models $^4$

--

- Review: Christin et al. - Applications of deep learning in ecology


.footnote[.small[
$^1$ Milo≈°eviƒá et al. (2019); $^2$ Schneider et al. (2018); $^3$ Kroodsma et al. (2019) ; $^4$ Joseph (2020)]]


???
_Most Papers very recently published in this field (> 2017)_
_Usage not only for image classification -> think outside of the box (Max Joseph)_

---

### Frameworks for training deep learning models

- <img src="Pictures/TensorFlowLogo.svg" style="width: 12%"/>
  - Library for training and inference of deep neural networks developed for internal use by Google (2011)
  - Written in *Python* and *C++*
  - R: https://tensorflow.rstudio.com/

--

- <img src="Pictures/Keras.png" style="width: 12%"/>
  - Open-source framework by Francois Chollet (written in *Python*)
  - Easy to use, supports also other deep learning libraries as backend 
  - Keras now part of TensorFlow

--

- <img src="Pictures/Pytorch.png" style="width: 16%"/>
  - Developd by Facebook in 2016 (open-source) for *Python*
  - Easy to use
  - R: torch package https://torch.mlverse.org/packages/

???
Keras: coding the model and the training, TensorFlow for high-performance data pipeline, 
provids also backend to other frameworks (e.g. Theano)


_Tensorflow source: https://upload.wikimedia.org/wikipedia/commons/1/11/TensorFlowLogo.svg_

_Mention that there are also frameworks for inference, e.g. to make predictions inside an app_

---

### Frameworks for training deep learning models

- <img src="Pictures/TensorFlowLogo.svg" style="width: 12%"/>

- <img src="Pictures/Keras.png" style="width: 12%"/>

- <img src="Pictures/Pytorch.png" style="width: 16%"/>


- Sharing models: Open Neural Network Exchange (ONNX)

???
ONNX: Standard format for machine learning models; provides conversion of models between frameworks
Computation graph models, definitions for operators and standard data types
Could use R with ONNX
There is also the https://github.com/rstudio/tensorflow and 
https://cran.r-project.org/web/packages/keras/vignettes/index.html
https://torch.mlverse.org/

---

### Transfer learning example in Python


- We are using an existing pretrained model *MobileNet()*

- This model is an CNN

- Its weights are trained on a large annotated image database (ImageNet)

- We will "freeze" most of the generic layers

- We will change few of the last layers (specific) to customize the model
for our classification task

---

### Applications

.center[

Art: 

<img src="Pictures/rbs.png" style="width: 20%"/>

]

--

.center[

<img src="Pictures/rbs_express.jpg" style="width: 20%"/>
]


???
_Sources for pictures: 
computer vision:
https://www.analyticsinsight.net/wp-content/uploads/2019/04/Computer-Vision12.png
speech recognition:
https://joelgarciajr84.medium.com/creating-an-application-that-uses-speech-recognition-76117a396b7d



---

## Additional 
### Basics of Deep Feedforward networks

- Goal: approximate some function $f$ 

--
- Mapping of some input x to an output y (e.g. a category)

$y = f(x, \theta)$ ; $\theta = parameters$

--
- Learns the value of the parameters $\theta$ that result in the best function approximation

--

- **Feedforward**: Information flows from $x$, through $f$ to output $y$

--

- **Networks**: Combine many different functions: 


$f^{(1)}, f^{(2)}, f^{(3)} \rightarrow f(x) f^{(3)}(f^{(2)}(f^{(1)}(x)))$

$f^{(1)} \rightarrow$ first layer, $f^{(2)} \rightarrow$ second layer, ... 

--

- Length of the chain: **Depth** of the model

--

- **Neural**: Inspired by the function of the brain, specifically the neuron

???
_No feedback connections in which outputs of the model are fed back into itself
_recurrent neural networks have feedback connections

_Deep Learnings vs shallow learning

_Only loosly inspired by neuroscience and the working of a neuron


---

### Basics of Deep forward networks 

Add here picture with Deep forward network structure (input, hidden and output layers)
then come to discussion about relationship to neuroscience

- Hidden layer: vectors
- Each element in the vector can be seen as a "neuron" (units)
- Each unit receives input from other units and computes its own activation value

- Conclusion: Rather think of feedforward networks as *function approximation machines* !

---

### Relationship to statistics

*"Neural networks are merely regression models with transformed predictors" (J. Hoeting)*

--

- Linear regression: 
  - Easy to fit, known assumptions
  - Limited to linear functions (cannot understand interaction between two input variables)

--
- Extension to represent nonlinear functions: apply linear model to transformed input 
$\phi(x)$; $\phi$ nonlinear transformation

- Nonparametric regression: Polynomial regression, regression splines
???
_splines apply transformation to x_
_Doesn't work well for many predictors_
_User has to decide about the transformation_

--

- Deep learning: learns $\phi$ based on the data!





